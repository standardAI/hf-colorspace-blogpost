<style>
  article {
    font-family: sans-serif;
    max-width: 1028px;
  }
  h1 {
    font-size: 3rem;
  }
  h2 {
    font-size: 2.5rem;
  }
  h3 {
    font-size: 1.5rem;
  }
  .side-by-side img {
    margin: 2px;
    max-width: 49%;
  }
  img {
    width: 512px;
  }
  pre {
    border-radius: 5px;
    background: #efefff;
    padding: 12px 0;
    max-width: 1028px;
  }
</style>
<article>
  <h1>Exploring the SDXL latent space</h1>
  <h3><a href="#implementation">TL;DR</a></h3>
  <h2>A short background story</h2>
  <p>
    I was creating <a href="youtu.be/jjdrhIgLDvQ">correction filters</a> for the SDXL inference process to an UI I'm creating for diffusion models.
  </p>

  <p>
    After having many years of experience with image correction, I wanted the fundamental capability to improve the actual output from SDXL.<br>
    There were many techniques which I wanted available in the UX, which I set out to fix myself.<br>
    I noticed that SDXL output is almost always either noisy in regular patterns or overly smooth.<br>
    The color space always needed white balancing, with a biased and restricted color range, simply because of how SD models work.
  </p>

  <p>
    Making corrections in a post process after the image is generated and converted to 8-bit RGB made very little sense, if it was possible to improve the information and color range before the actual output.
  </p>

  <p>
    The most important thing to know in order to create filters and correction tools is to understand the data you are working with.
  </p>

  <p>
    This led me to an experimental exploration of the SDXL latents with the intention of understanding them.<br>
    The tensor, which the diffusion models based on the SDXL architecture work with, looks like this:<br>
    [batch_size, 4 channels, height (y), width (x)]
  </p>

  <p>
    My first question was simply "What exactly are these 4 channels?".<br>
    To which most answers I received were along the lines of "It's not something that a human can understand."
  </p>

  <p>
    But it is most definitely understandable. It's even very easy to understand and useful to know.
  </p>

  <h2>The 4 channels of the SDXL tensor</h2>
  <p>
    For a 3x1024×1024px image generated by SDXL, the latents tensor is 4x128×128px, where every pixel in the latent space represents 48 (8×8x3/4) pixels in the pixel space. If we generate and decode the latents into a standard 8-bit jpg image, then...
  </p>
  <h3>In the pixel space, we have 3 channels</h3>
  <p>
    Red (R), Green (G) and Blue (B), each with 256 possible values ranging between 0-255.<br>
    So, to store the full information of 48 pixels, we need to be able to store 48×256 = 12,288 values, per channel, in every latent pixel.
  </p>

  <h3>In the SDXL latent representation of an image, we have 4 channels</h3>
  <p>
    <strong>0:</strong> Luminance<br>
    <strong>1:</strong> Cyan/Red, equivalent to rgb(0, 255, 255)/rgb(255, 0, 0)<br>
    <strong>2:</strong> Lime/Medium Purple, equivalent to rgb(127, 255, 0)/rgb(127, 0, 255)<br>
    <strong>3:</strong> Pattern/structure.<br>
  </p>
  <p>
    If each value can range between -4 and 4 at the point of decoding, then in a 16-bit floating point format with half precision, each latent pixel can contain 12,288 distinct values for each of the 4 channels.
  </p>


  <p>With this understanding, we can create an approximation function which directly converts the latents to RGB:</p>
  <pre>
  def latents_to_rgb(latents):
      weights = (
          (60, -60, 25, -70),
          (60,  -5, 15, -50),
          (60,  10, -5, -35)
      )

      weights_tensor = torch.t(torch.tensor(weights, dtype=latents.dtype).to(latents.device))
      biases_tensor = torch.tensor((150, 140, 130), dtype=latents.dtype).to(latents.device)
      rgb_tensor = torch.einsum("...lxy,lr -> ...rxy", latents, weights_tensor) + biases_tensor.unsqueeze(-1).unsqueeze(-1)
      image_array = rgb_tensor.clamp(0, 255)[0].byte().cpu().numpy()
      image_array = image_array.transpose(1, 2, 0)  # Change the order of dimensions

      return Image.fromarray(image_array)</pre>
  <p>
    Here we have the latents_to_rgb result and a regular decoded output, resized for comparison:
  </p>
  <div class="side-by-side">
    <img src="latents_rgb_approximation.png" style="width: 128px" />
    <img src="latents_rgb_dedoded.png" style="width: 128px" />
  </div>

  <p>
    Relatively few things in nature are blue, or white. These colors are most prominent in the sky, during enjoyable conditions.<br>
    So, the model, knowing reality through images, thinks in cyan/red (channel 1) and lime/medium purple (channel 2), where Red and Green are primary and blue is secondary. This is why many SDXL generations are biased towards yellow (red + green).
  </p>

  <p>
    During inference, the values in the tensor will begin at <strong>min < -30</strong> and <strong>max > 30</strong> and the min/max boundary at time of decoding is around -4 to +4.
    At higher <strong>guidance_scale</strong> the values will have a higher difference between min and max.
  </p>

  <p>
    One key in understanding the boundary is to look at what happens in the decoding process:
  </p>
  <pre>
  decoded = vae.decode(latents / vae.scaling_factor).sample # (SDXL vae.scaling_factor = 0.13025)
  decoded = decoded.div(2).add(0.5).clamp(0, 1) # The dynamics outside of 0 to 1 at this point will be lost</pre>

  <p>
    If the values at this point are outside of the range 0 to 1, some information will be lost in the clamp.<br>
    So if we can make corrections during denoising to serve the VAE what it expects, we may get better results.
  </p>

  <h2>What needs correcting</h2>
  <p>
    <strong>How do you sharpen a blurry image, white balance, improve detail, increase contrast or increase the color range?</strong><br>
    The best way is to <em>begin</em> with a sharp image, which is correctly white balanced with great contrast, crisp details and a high range.<br>
  </p>
  <p>
    It's far easier to blur a sharp image, shift the color balance, reduce contrast, get nonsensical details and limit the color range than to improve it.
  </p>
  <p>
    SDXL has a very prominent tendency to color bias and put values outside of the actual boundaries (left image). Which is easily solved by centering the values and getting them within the boundaries (right image):
  </p>
  <div class="side-by-side">
    <img src="color_correction_original.jpg" />
    <img src="color_correction_fixed.jpg" />
  </div>
  <pre>
  def center_tensor(input_tensor, per_channel_shift=1, full_tensor_shift=1, channels=[0, 1, 2, 3]):
    for channel in channels:
        input_tensor[0, channel] -= input_tensor[0, channel].mean() * per_channel_shift
    return input_tensor - input_tensor.mean() * full_tensor_shift</pre>
  <h3>
    Let's take an example output from SDXL:
  </h3>
  <p>
    <em>
      <strong>seed:</strong> 77777777
    </em><br>
    <em>
      <strong>guidance_scale:</strong> 20
    </em><br>
    <em>
      <strong>steps with base:</strong> 23
    </em><br>
    <em>
      <strong>steps with refiner:</strong> 10
    </em><br>
    <em>
      <strong>prompt:</strong> Cinematic.Beautiful smile action woman in detailed white mecha gundam armor with red details,green details,blue details,colorful,star wars universe,lush garden,flowers,volumetric lighting,perfect eyes,perfect teeth,blue sky,bright,intricate details,extreme detail of environment,infinite focus,well lit,interesting clothes,radial gradient fade,directional particle lighting,wow
    </em><br>
    <em>
      <strong>negative_prompt:</strong> helmet, bokeh, painting, artwork, blocky, blur, ugly, old, boring, photoshopped, tired, wrinkles, scar, gray hair, big forehead, crosseyed, dumb, stupid, cockeyed, disfigured, crooked, blurry, unrealistic, grayscale, bad anatomy, unnatural irises, no pupils, blurry eyes, dark eyes, extra limbs, deformed, disfigured eyes, out of frame, no irises, assymetrical face, broken fingers, extra fingers, disfigured hands
    </em><br>
  </p>
  <p>
    Note that I've purposely chosen a high guidance scale. How can we fix this? It's half painting, half photograph.<br>
    The colors range is biased towards yellow. To the right is a fixed generation with the exact same settings.
  </p>
  <div class="side-by-side">
    <img src="mecha_lady_original.jpg" />
    <img src="mecha_lady_fixed.jpg" />
  </div>
  <p>
    At guidance scale 10, the original is less crazy. But the fixed output takes care of the nonsensical details and corrects the color balance.
  </p>
  <div class="side-by-side">
    <img src="mecha_lady_10_original.jpg" />
    <img src="mecha_lady_10_fixed.jpg" />
  </div>
  <p>
    And with a guidance scale of 7.5, we can still conclude that the fixed output is better as that "smile" is easily fixed.
  </p>
  <div class="side-by-side">
    <img src="mecha_lady_sensible_original.jpg" />
    <img src="mecha_lady_sensible_fixed.jpg" />
  </div>
  <p>
    There are many things we can do in the latent space to generally improve a generation and there are some very simple things which we can do to target specific errors in a generation:<br>
    <ol>
      <li><strong>Outlier removal: </strong>This will control the amount of nonsensical details, by pruning values that are the farthest from the mean of the distribution. It also helps in generating at higher guidance_scale.</li>
      <li><strong>Color balancing and increased range: </strong>I have two main methods of achieving this. The first one is to shrink towards the mean while normalizing the values (Which will also remove outliers) and the second is to fix when the values get biased towards some color. This also helps in generating at higher guidance_scale.</li>
      <li><strong>Tensor maximizing: </strong>This is basically done by multiplying the tensors by a very small amount like 1e-5 for a few steps and to make sure that the final tensor is using the full possible range (up to -4/4) before converting to RGB. Remember, in the pixel space, it's easier to reduce contrast, saturation and sharpness with intact dynamics than to increase it.</li>
    </ol>
  </p>
  <strong>This simple implementation of the three methods are used in the last set of images, with <a href="#women_in_garden">the women in the garden.</a></strong>
  <h3>Shrinking towards the mean (will also remove outliers)</h3>
  <pre>
  def soft_clamp_tensor(input_tensor, threshold=3.5, boundary=4):
      if max(abs(input_tensor.max()), abs(input_tensor.min())) < 4:
          return input_tensor
      channel_dim = 1

      max_vals = input_tensor.max(channel_dim, keepdim=True)[0]
      max_replace = ((input_tensor - threshold) / (max_vals - threshold)) * (boundary - threshold) + threshold
      over_mask = (input_tensor > threshold)

      min_vals = input_tensor.min(channel_dim, keepdim=True)[0]
      min_replace = ((input_tensor + threshold) / (min_vals + threshold)) * (-boundary + threshold) - threshold
      under_mask = (input_tensor < -threshold)

      return torch.where(over_mask, max_replace, torch.where(under_mask, min_replace, input_tensor))</pre>
  <h3>Center tensor (balance colors)</h3>
  <pre>
  def center_tensor(input_tensor, channel_shift=1, full_shift=1, channels=[0, 1, 2, 3]):
      for channel in channels:
          input_tensor[0, channel] -= input_tensor[0, channel].mean() * channel_shift
      return input_tensor - input_tensor.mean() * full_shift</pre>
  <h3>Maximize/normalize tensor</h3>
  <pre>
  def maximize_tensor(input_tensor, boundary=4):
      min_val = input_tensor.min()
      max_val = input_tensor.max()

      normalization_factor = boundary / max(abs(min_val), abs(max_val))

      return input_tensor * normalization_factor</pre>
  <h3>Which can be used like this:</h3>
  <pre>
  def callback(pipe, step_index, timestep, cbk):
      if timestep > 950:
          threshold = max(cbk["latents"].max(), abs(cbk["latents"].min())) * 0.998
          cbk["latents"] = soft_clamp_tensor(cbk["latents"], threshold*0.998, threshold)
      if timestep > 700:
          cbk["latents"] = center_tensor(cbk["latents"], 0.8, 0.8)
      if timestep > 1 and timestep < 100:
          cbk["latents"] = center_tensor(cbk["latents"], 0.6, 1.0)
          cbk["latents"] = maximize_tensor(cbk["latents"])
      return cbk

  image = base(
      prompt,
      guidance_scale = guidance_scale,
      callback_on_step_end=callback,
      callback_on_step_end_inputs=["latents"]
  ).images[0]</pre>
  <h2 id="implementation">
    <a href="https://timothyalexisvass.github.io/sdxl-correction/">Here we have a full demonstration</a>
  </h2>
  (To check a wide range of results with this example, click the link in the heading)
  <h3>Original (too yellow) and slight modification (white balanced)</h3>
  <div class="side-by-side">
    <img src="happy_meadow_original_sdxl.jpg" />
    <img src="happy_meadow_lowfix_sdxl.jpg" />
  </div>
  <h3>Medium modification and hard modification (both with all 3 techniques applied)</h3>
  <div class="side-by-side">
    <img src="happy_meadow_mediumfix_sdxl.jpg" />
    <img src="happy_meadow_hardfix_sdxl.jpg" />
  </div>
  </h3>

  <h2>Increasing color range / removing color bias</h2>
  <p>
    For image below, SDXL has already limited the color range to red and green in the regular output. Because there is nothing in the prompt suggesting that there is such a thing as blue. This is a rather good generation, but the color range has become restricted.
  </p>
  <p>
    If you give someone a palette of black, red, green and yellow and then tell them to paint a clear blue sky, the natural response is to ask you to supply blue and white.
  </p>
  <p>
    To include blue in the generation, we can simply realign the color space when it gets restricted and SDXL will appropriately include the full color spectrum in the generation.
  </p>
  <div class="side-by-side">
    <img src="strawberry_original.jpg" />
    <img src="strawberry_fixed.jpg" />
  </div>

  <h2 id="women_in_garden">Long prompts at high guidance scales becoming possible</h2>
  <p>
    Here is a typical scenario, where the increased color range makes the <strong>whole</strong> prompt possible.<br>
    This example apply a generic high modification, to illustrate the difference more clearly.
  </p>
  <p>
    <em>
      <strong>prompt:</strong> Photograph of woman in red dress in a luxury garden surrounded with <strong style="color: red">blue</strong>, yellow, purple and flowers in <strong style="color: red">many colors</strong>, high class, award-winning photography, Portra 400, full format. <strong style="color: red">blue sky</strong>, intricate details even to the smallest particle, extreme detail of the environment, sharp portrait, <strong style="color: red">well lit</strong>, <strong style="color: red">interesting</strong> outfit, beautiful shadows, <strong style="color: red">bright</strong>, photoquality, ultra realistic, masterpiece
    </em>
  </p>
  <div class="side-by-side">
    <img src="sdxl_garden_original.jpg" />
    <img src="sdxl_garden_fixed.jpg" />
  </div>

  <h3>Some more examples with the same concept and modifying settings, but different seeds</h3>
  <div class="side-by-side">
    <img src="sdxl_garden_original1.jpg" />
    <img src="sdxl_garden_fixed1.jpg" />
  </div>
  <div class="side-by-side">
    <img src="sdxl_garden_original2.jpg" />
    <img src="sdxl_garden_fixed2.jpg" />
  </div>
  <div class="side-by-side">
    <img src="sdxl_garden_original3.jpg" />
    <img src="sdxl_garden_fixed3.jpg" />
  </div>
  <div class="side-by-side">
    <img src="sdxl_garden_original4.jpg" />
    <img src="sdxl_garden_fixed4.jpg" />
  </div>
  <div class="side-by-side">
    <img src="sdxl_garden_original5.jpg" />
    <img src="sdxl_garden_fixed5.jpg" />
  </div>
</article>
